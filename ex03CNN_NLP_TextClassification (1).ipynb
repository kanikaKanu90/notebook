{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_NLP_TextClassification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20WaoJnQ3Jpx"
      },
      "source": [
        "## Exercise 3 - Language Identification -Reloaded and Convoluted"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79dccq9UzSUL"
      },
      "source": [
        "\n",
        "#Importing the tools\n",
        "import csv\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F751peA-3T9s"
      },
      "source": [
        "\n",
        "Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRoE3MrNZOyP"
      },
      "source": [
        "url_train_dev = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vTOZ2rC82rhNsJduoyKYTsVeH6ukd7Bpxvxn_afOibn3R-eadZGXu82eCU9IRpl4CK_gefEGsYrA_oM/pub?gid=1863430984&single=true&output=tsv'\n",
        "url_test = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vT-KNR9nuYatLkSbzSRgpz6Ku1n4TN4w6kKmFLkA6QJHTfQzmX0puBsLF7PAAQJQAxUpgruDd_RRgK7/pub?gid=417546901&single=true&output=tsv'"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMDCE4Km3d4K"
      },
      "source": [
        "Splitting the data into tweet and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "li6RIPkiZYMh"
      },
      "source": [
        "\n",
        "from io import StringIO\n",
        "import requests\n",
        "\n",
        "def load_dataset(url):\n",
        "    r = requests.get(url)\n",
        "    data = r.content.decode('utf8')\n",
        "    df = pd.read_csv(StringIO(data), sep='\\t')\n",
        "    df.columns = ['tweet', 'label']\n",
        "    return df\n",
        "    "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtpP57DAZfhr",
        "outputId": "27091eb8-2d6e-46c5-fec8-863d30780376",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df_train  = load_dataset(url_train_dev)\n",
        "df_test = load_dataset(url_test)\n",
        "print(df_train[0:10])\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                               tweet label\n",
            "0  يا من أناديها ويخنقني البكاء  ويكاد صمت الدمع ...    ar\n",
            "1  فيه فرق بين اهل غزة اللى مطحونين من ناحيتين وب...    ar\n",
            "2  ﻋﻦ ﺍﻟﻠﺤﻈﺔ اﻟﺤﻠﻮﺓﺓ ﺍﻟﻠﻲ ﺑﺘﻐﻤﺾ ﻓﻴﻬﺎ ﻋﻴﻨﻴﻚ ﺑﺘﻔﻜﺮ ...    ar\n",
            "3                                  يا ابو سلو عرفتني    ar\n",
            "4  ب50 ريال أكفل معتمر في رمضان ، ولك بإذن الله م...    ar\n",
            "5  توجيه كيفية تثبيت البرامج الثابتة ROM التحميل ...    ar\n",
            "6  {وأنه هو أغنى وأقنى} [النجم:48] http://t.co/is...    ar\n",
            "7  اللهم قدر لنا الفرح بكل اشكاله ، انت الكريم ال...    ar\n",
            "8  #غزه_تحت_القصف  داعش أخواني حيل عندكم بالمدنيي...    ar\n",
            "9  {يعلمون ظاهرا من الحياة الدنيا وهم عن الآخرة ه...    ar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDHUPsvA3mnT"
      },
      "source": [
        "Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wCuI6Z0i0hO"
      },
      "source": [
        "#Converting all the tweets to lower string\n",
        "df_train[\"tweet\"] = df_train[\"tweet\"].str.lower()\n",
        "filter_dt=df_train.groupby('label').count()['tweet']\n",
        "\n",
        "#Removing all tweets with less than 15 samples to train from \n",
        "filter_dt=filter_dt.loc[filter_dt.values>15].index\n",
        "df_train=df_train[df_train['label'].isin(filter_dt)]\n",
        "df_train.reset_index(inplace=True)\n",
        "filter_dt=df_train.groupby('label').count()['tweet']\n",
        "\n",
        "#Removing all labels from test which are not there in training\n",
        "df_test=df_test[df_test['label'].isin(df_train.label.unique())]\n",
        "df_test.reset_index(inplace=True)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "web4bUhzbyPU",
        "outputId": "8ccf2078-555a-40d4-c969-8dccc0893dfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df_train.info()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 52460 entries, 0 to 52459\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   index   52460 non-null  int64 \n",
            " 1   tweet   52460 non-null  object\n",
            " 2   label   52460 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 1.2+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAZRzNgGdxQQ",
        "outputId": "03b6c215-524a-43b9-9df7-fef8a0cfa06c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>يا من أناديها ويخنقني البكاء  ويكاد صمت الدمع ...</td>\n",
              "      <td>ar</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>فيه فرق بين اهل غزة اللى مطحونين من ناحيتين وب...</td>\n",
              "      <td>ar</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>ﻋﻦ ﺍﻟﻠﺤﻈﺔ اﻟﺤﻠﻮﺓﺓ ﺍﻟﻠﻲ ﺑﺘﻐﻤﺾ ﻓﻴﻬﺎ ﻋﻴﻨﻴﻚ ﺑﺘﻔﻜﺮ ...</td>\n",
              "      <td>ar</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>يا ابو سلو عرفتني</td>\n",
              "      <td>ar</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>ب50 ريال أكفل معتمر في رمضان ، ولك بإذن الله م...</td>\n",
              "      <td>ar</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index                                              tweet label\n",
              "0      0  يا من أناديها ويخنقني البكاء  ويكاد صمت الدمع ...    ar\n",
              "1      1  فيه فرق بين اهل غزة اللى مطحونين من ناحيتين وب...    ar\n",
              "2      2  ﻋﻦ ﺍﻟﻠﺤﻈﺔ اﻟﺤﻠﻮﺓﺓ ﺍﻟﻠﻲ ﺑﺘﻐﻤﺾ ﻓﻴﻬﺎ ﻋﻴﻨﻴﻚ ﺑﺘﻔﻜﺮ ...    ar\n",
              "3      3                                  يا ابو سلو عرفتني    ar\n",
              "4      4  ب50 ريال أكفل معتمر في رمضان ، ولك بإذن الله م...    ar"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVBZBK_id1mH",
        "outputId": "88b264b3-1fcc-4dac-ab54-e3a3ecff1a89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df_train.label.unique()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['ar', 'ca', 'de', 'el', 'en', 'es', 'fa', 'fr', 'he', 'hi', 'id',\n",
              "       'it', 'ja', 'ko', 'ms', 'nl', 'pl', 'pt', 'ru', 'sr', 'sv', 'th',\n",
              "       'tl', 'tr', 'uk', 'und', 'vi', 'zh-CN'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE_P7qH64A5r"
      },
      "source": [
        "Retrieving the tweets and labels from the training and the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1kckFNrR4xl",
        "outputId": "c3b1cb80-83af-4a25-8894-e90086353158",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X_train = df_train.tweet\n",
        "y_train = df_train.label\n",
        "X_test = df_test.tweet\n",
        "y_test = df_test.label\n",
        "\n",
        "print('Training tweet shape: ', X_train.shape)\n",
        "print('Training labels shape: ', y_train.shape)\n",
        "print('Test tweet shape: ', X_test.shape)\n",
        "print('Test labels shape: ', y_test.shape)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training tweet shape:  (52460,)\n",
            "Training labels shape:  (52460,)\n",
            "Test tweet shape:  (13217,)\n",
            "Test labels shape:  (13217,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lF_AYCZ4LH4"
      },
      "source": [
        "Data Cleaning - removing all the unnecessary data from the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaNT0WGT7apG",
        "outputId": "aaf625b6-7723-4a73-d32a-149069c007dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def preprocessor(text):\n",
        "  \n",
        "  text = re.sub(r\"http\\S+\", \"\", text)       ##Removing URL\n",
        "  text = re.sub('@[^\\s]+','',text)          ##Removing @Username\n",
        "  text = re.sub('[\\W]+', ' ', text.lower()) ##Removing NonCharacter eg.emojis\n",
        "  text = re.sub('<[^>]*>', '', text)        ##Removing entire HTML\n",
        "  text = re.sub(r'[0-9]+', '', text)        ## Removing numbers\n",
        "  \n",
        "  return text\n",
        "\n",
        "#Applying the text cleaning to the train data\n",
        "X_train = X_train.apply(preprocessor)\n",
        "X_train[0:30] #Checking the data "
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     يا من أناديها ويخنقني البكاء ويكاد صمت الدمع أ...\n",
              "1     فيه فرق بين اهل غزة اللى مطحونين من ناحيتين وب...\n",
              "2     ﻋﻦ ﺍﻟﻠﺤﻈﺔ اﻟﺤﻠﻮﺓﺓ ﺍﻟﻠﻲ ﺑﺘﻐﻤﺾ ﻓﻴﻬﺎ ﻋﻴﻨﻴﻚ ﺑﺘﻔﻜﺮ ...\n",
              "3                                     يا ابو سلو عرفتني\n",
              "4     ب ريال أكفل معتمر في رمضان ولك بإذن الله مثل أ...\n",
              "5     توجيه كيفية تثبيت البرامج الثابتة rom التحميل ...\n",
              "6                            وأنه هو أغنى وأقنى النجم  \n",
              "7     اللهم قدر لنا الفرح بكل اشكاله انت الكريم الذي...\n",
              "8      غزه_تحت_القصف داعش أخواني حيل عندكم بالمدنيين...\n",
              "9      يعلمون ظاهرا من الحياة الدنيا وهم عن الآخرة ه...\n",
              "10               افضل كتاب قرأته هو أمي ابراهام لنكولن \n",
              "11    ولأن ه م م لائ ك ة ص غار ن ع ش ق ات كاءة رؤوس ...\n",
              "12        خ لاصة الح ب هي ت فكر بقلبهآ وهو ي فكر بعقله \n",
              "13    جميل آن يفهمك من تحبب ويخآف عليك و يغآر عليكك ...\n",
              "14    حتى الندم على المعصيه تؤجر عليه سبحانك يالله م...\n",
              "15     اها يا بيبي والله اتهرست علي تويتر و ع الفيس ...\n",
              "16     لا يقاتلونكم جميعا إلا في قرى محصنة أو من ورا...\n",
              "17          طبت منك نهائيا بس كلي رجاء ما ترجع من جديد \n",
              "18     وإن تجهر بالقول فإنه يعلم السر وأخفى طه  تطبي...\n",
              "19                                                  الو\n",
              "20             داعش يصلب سوريا  ساعات وهو حي داعش syria\n",
              "21    ﺧﻟك ﻋزﯾز آﻟﻧﻓس ﻟۈ ﮪﻣۈﻣك ﺟﺑآللاﭠﺷﺷﮐي ﻟﻟﻧآﺳس ﻣن ...\n",
              "22    اللهم أفرح قلبي وقلب من أحب وأغسل أحزاننا وهمو...\n",
              "23    اللهم أحسن عاقبتنا في الأمور كلها وأجرنا من خز...\n",
              "24     سيدي إيفني شكاية أجنبي بدرك ميرلفت يحرك فرقة ...\n",
              "25     الصوره هاذي الله يسلمك في اسبانبا غير صحيح ان...\n",
              "26                             انتظري اجل خيره لك يارب \n",
              "27               اللهم اسعد امي كما تسعدني و احفظها لي \n",
              "28     إن الذين كفروا من أهل الكتاب والمشركين في نار...\n",
              "29    اللهم أعز الإسلآم والمسلمين اللهم زلزل الأرض م...\n",
              "Name: tweet, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqhdA7Zw4bs6"
      },
      "source": [
        "Checking the shape and assigning new variables to train and test data to be used for further processing of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3K1zxZpT1r_y",
        "outputId": "c4d12724-e93d-426d-f312-3d95773f6cfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "X_train_n = X_train\n",
        "y_train_n = y_train\n",
        "X_test_n = X_test\n",
        "y_test_n = y_test\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(52460,)\n",
            "(52460,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTcwd_to42wL"
      },
      "source": [
        "Preparing the final data for the model\n",
        "\n",
        "\n",
        "1.   Tokenizing text data\n",
        "2.   Encoding labels\n",
        "3.   Finding the maximum length of the data and vocab size  for the input shape of the model input layer\n",
        "4.   Printing the details\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycPjJbdy2qvj",
        "outputId": "de479ff3-dae5-43a4-da62-41adf8c1e42f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer                    \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout,MaxPooling1D, Conv1D, Flatten , Embedding\n",
        "from keras.preprocessing import text, sequence\n",
        "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
        "from keras import utils\n",
        "\n",
        "#Calulating the length of the doc to be fed into the input layer of the model\n",
        "def maxilen(lines):\n",
        "\treturn max([len(s.split()) for s in lines])\n",
        "\n",
        "#Fitting the dataTokenizer\n",
        "def Generatetokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "#Creating the tokenizer\n",
        "tokenizer = Generatetokenizer(X_train_n[0:])\n",
        "\n",
        "#Encoding Text\n",
        "def text_encoder(tokenizer, lines, length):\n",
        "\t\n",
        "\tencoded = tokenizer.texts_to_sequences(lines)\n",
        "\tpadded = pad_sequences(encoded, maxlen=length, padding='post')\n",
        "\treturn padded\n",
        "\n",
        "#Calculating the maximum document length for the input shape for the model input\n",
        "length = maxilen(X_train_n[0:])\n",
        "\n",
        "#Calculating the vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "print('Maximum document length: %d' % length)\n",
        "print('Vocabulary size of the data: %d' % vocab_size)\n",
        "\n",
        "#Encoding the data\n",
        "trainX = text_encoder(tokenizer, X_train_n[0:], length)\n",
        "testX = text_encoder(tokenizer, X_test_n[0:], length)\n",
        "print(trainX.shape, testX.shape)\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(y_train_n)\n",
        "y_trainning_labeled = encoder.transform(y_train_n)\n",
        "y_testing_labeled = encoder.transform(y_test_n)\n",
        "\n",
        "num_classes = np.max(y_trainning_labeled) + 1\n",
        "y_trainning_labeled = utils.to_categorical(y_trainning_labeled, num_classes)\n",
        "y_testing_labeled = utils.to_categorical(y_testing_labeled, num_classes)\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum document length: 1224\n",
            "Vocabulary size of the data: 102072\n",
            "(52460, 1224) (13217, 1224)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7leQdjdb6H3o"
      },
      "source": [
        "Splitting the data to 90/10 - test/validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qTDpJ1M5__t"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train_n , X_val , y_train_n , y_val = train_test_split (trainX, y_trainning_labeled, test_size= 0.1,random_state=42)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1qg7Z2q9zy1",
        "outputId": "8a12771c-47f9-438c-b959-11344078d541",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X_train_n.shape"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(47214, 1224)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52Oy6qtk5qnC"
      },
      "source": [
        "Creating model using Keras and implementing different layers \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABgZWttkfibN",
        "outputId": "095fc374-ef42-4a4a-bb0b-04138c3f33db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100, input_length=length))\n",
        "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "print(model.summary())\n",
        "\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 1224, 100)         10207200  \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 1222, 128)         38528     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1 (None, 611, 128)          0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 611, 128)          0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 78208)             0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 28)                2189852   \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 28)                0         \n",
            "=================================================================\n",
            "Total params: 12,435,580\n",
            "Trainable params: 12,435,580\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lvTFVpn7DRB"
      },
      "source": [
        "Fitting the model to split train data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFs0QGG4yms8",
        "outputId": "57be4cb5-bb3a-455a-b2a2-c40043e3ffe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "history = model.fit(X_train_n, y_train_n,\n",
        "                    epochs=5,batch_size=32)  \n",
        "\n",
        "# save the model\n",
        "model.save('model.h5')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1476/1476 [==============================] - 152s 103ms/step - loss: 0.0242 - accuracy: 0.9939\n",
            "Epoch 2/5\n",
            " 388/1476 [======>.......................] - ETA: 1:52 - loss: 0.0119 - accuracy: 0.9968"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-a64e869bdcd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m history = model.fit(X_train_n, y_train_n,\n\u001b[0;32m----> 3\u001b[0;31m                     epochs=5,batch_size=32)  \n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# save the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}
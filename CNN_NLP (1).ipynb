{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ex01_lc.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "79dccq9UzSUL"
      },
      "source": [
        "#Part 1 - Language identification with linear classification [ SGDClassifier and MultinomialNB ]\n",
        "\n",
        "\n",
        "#Importing the tools\n",
        "import csv\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRoE3MrNZOyP"
      },
      "source": [
        "#Loading the Data\n",
        "url_train_dev = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vTOZ2rC82rhNsJduoyKYTsVeH6ukd7Bpxvxn_afOibn3R-eadZGXu82eCU9IRpl4CK_gefEGsYrA_oM/pub?gid=1863430984&single=true&output=tsv'\n",
        "url_test = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vT-KNR9nuYatLkSbzSRgpz6Ku1n4TN4w6kKmFLkA6QJHTfQzmX0puBsLF7PAAQJQAxUpgruDd_RRgK7/pub?gid=417546901&single=true&output=tsv'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "li6RIPkiZYMh"
      },
      "source": [
        "#Splitting Data into labels\n",
        "from io import StringIO\n",
        "import requests\n",
        "\n",
        "def load_dataset(url):\n",
        "    r = requests.get(url)\n",
        "    data = r.content.decode('utf8')\n",
        "    df = pd.read_csv(StringIO(data), sep='\\t')\n",
        "    df.columns = ['tweet', 'label']\n",
        "    return df\n",
        "    "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtpP57DAZfhr",
        "outputId": "d3f12b90-a61b-4da3-c218-e7e20556066e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df_train  = load_dataset(url_train_dev)\n",
        "df_test = load_dataset(url_test)\n",
        "print(df_train[0:10])\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                               tweet label\n",
            "0  يا من أناديها ويخنقني البكاء  ويكاد صمت الدمع ...    ar\n",
            "1  فيه فرق بين اهل غزة اللى مطحونين من ناحيتين وب...    ar\n",
            "2  ﻋﻦ ﺍﻟﻠﺤﻈﺔ اﻟﺤﻠﻮﺓﺓ ﺍﻟﻠﻲ ﺑﺘﻐﻤﺾ ﻓﻴﻬﺎ ﻋﻴﻨﻴﻚ ﺑﺘﻔﻜﺮ ...    ar\n",
            "3                                  يا ابو سلو عرفتني    ar\n",
            "4  ب50 ريال أكفل معتمر في رمضان ، ولك بإذن الله م...    ar\n",
            "5  توجيه كيفية تثبيت البرامج الثابتة ROM التحميل ...    ar\n",
            "6  {وأنه هو أغنى وأقنى} [النجم:48] http://t.co/is...    ar\n",
            "7  اللهم قدر لنا الفرح بكل اشكاله ، انت الكريم ال...    ar\n",
            "8  #غزه_تحت_القصف  داعش أخواني حيل عندكم بالمدنيي...    ar\n",
            "9  {يعلمون ظاهرا من الحياة الدنيا وهم عن الآخرة ه...    ar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wCuI6Z0i0hO"
      },
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "df_train[\"tweet\"] = df_train[\"tweet\"].str.lower()\n",
        "filter_dt=df_train.groupby('label').count()['tweet']\n",
        "\n",
        "\n",
        "#Removing all tweets with less than 15 samples to train from \n",
        "filter_dt=filter_dt.loc[filter_dt.values>25].index\n",
        "df_train=df_train[df_train['label'].isin(filter_dt)]\n",
        "df_train.reset_index(inplace=True)\n",
        "filter_dt=df_train.groupby('label').count()['tweet']\n",
        "\n",
        "#Removing all labels from test which are not there in training\n",
        "df_test=df_test[df_test['label'].isin(df_train.label.unique())]\n",
        "df_test.reset_index(inplace=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "web4bUhzbyPU",
        "outputId": "a7ba241b-1bd2-4bd5-d85f-431766e6c913",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df_train.info()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 52325 entries, 0 to 52324\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   index   52325 non-null  int64 \n",
            " 1   tweet   52325 non-null  object\n",
            " 2   label   52325 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 1.2+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAZRzNgGdxQQ",
        "outputId": "64fbdd5b-0297-4891-fa31-f359034ff001",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>يا من أناديها ويخنقني البكاء  ويكاد صمت الدمع ...</td>\n",
              "      <td>ar</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>فيه فرق بين اهل غزة اللى مطحونين من ناحيتين وب...</td>\n",
              "      <td>ar</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>ﻋﻦ ﺍﻟﻠﺤﻈﺔ اﻟﺤﻠﻮﺓﺓ ﺍﻟﻠﻲ ﺑﺘﻐﻤﺾ ﻓﻴﻬﺎ ﻋﻴﻨﻴﻚ ﺑﺘﻔﻜﺮ ...</td>\n",
              "      <td>ar</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>يا ابو سلو عرفتني</td>\n",
              "      <td>ar</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>ب50 ريال أكفل معتمر في رمضان ، ولك بإذن الله م...</td>\n",
              "      <td>ar</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index                                              tweet label\n",
              "0      0  يا من أناديها ويخنقني البكاء  ويكاد صمت الدمع ...    ar\n",
              "1      1  فيه فرق بين اهل غزة اللى مطحونين من ناحيتين وب...    ar\n",
              "2      2  ﻋﻦ ﺍﻟﻠﺤﻈﺔ اﻟﺤﻠﻮﺓﺓ ﺍﻟﻠﻲ ﺑﺘﻐﻤﺾ ﻓﻴﻬﺎ ﻋﻴﻨﻴﻚ ﺑﺘﻔﻜﺮ ...    ar\n",
              "3      3                                  يا ابو سلو عرفتني    ar\n",
              "4      4  ب50 ريال أكفل معتمر في رمضان ، ولك بإذن الله م...    ar"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVBZBK_id1mH",
        "outputId": "f48eb617-ddd9-4e8a-8774-a360a43523e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df_train.label.unique()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['ar', 'de', 'el', 'en', 'es', 'fr', 'he', 'id', 'it', 'ja', 'ko',\n",
              "       'ms', 'nl', 'pl', 'pt', 'ru', 'sv', 'th', 'tl', 'tr', 'und'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1kckFNrR4xl",
        "outputId": "aec08344-f18b-45c5-810f-b007f13c015d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Retrieving the labels from the training and the test set.\n",
        "X_train = df_train.tweet\n",
        "y_train = df_train.label\n",
        "X_test = df_test.tweet\n",
        "y_test = df_test.label\n",
        "\n",
        "print('Training tweet shape: ', X_train.shape)\n",
        "print('Training labels shape: ', y_train.shape)\n",
        "print('Test tweet shape: ', X_test.shape)\n",
        "print('Test labels shape: ', y_test.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training tweet shape:  (52325,)\n",
            "Training labels shape:  (52325,)\n",
            "Test tweet shape:  (13190,)\n",
            "Test labels shape:  (13190,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaNT0WGT7apG"
      },
      "source": [
        "#Data Cleaning\n",
        "def preprocessor(text):\n",
        "  \n",
        "  text = re.sub(r\"http\\S+\", \"\", text)       ##Removing URL\n",
        "  text = re.sub('@[^\\s]+','',text)          ##Removing @Username\n",
        "  text = re.sub('[\\W]+', ' ', text.lower()) ##Removing NonCharacter eg.emojis\n",
        "  text = re.sub('<[^>]*>', '', text)        ##Removing entire HTML\n",
        "  text = re.sub(r'[0-9]+', '', text)        ## Removing numbers\n",
        "  \n",
        "  return text\n",
        "\n",
        "X_train = X_train.apply(preprocessor)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_Bxh9kER3-n",
        "outputId": "ef156199-0d1c-49a9-e378-71f2e78c14c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X_train[0:50]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     يا من أناديها ويخنقني البكاء ويكاد صمت الدمع أ...\n",
              "1     فيه فرق بين اهل غزة اللى مطحونين من ناحيتين وب...\n",
              "2     ﻋﻦ ﺍﻟﻠﺤﻈﺔ اﻟﺤﻠﻮﺓﺓ ﺍﻟﻠﻲ ﺑﺘﻐﻤﺾ ﻓﻴﻬﺎ ﻋﻴﻨﻴﻚ ﺑﺘﻔﻜﺮ ...\n",
              "3                                     يا ابو سلو عرفتني\n",
              "4     ب ريال أكفل معتمر في رمضان ولك بإذن الله مثل أ...\n",
              "5     توجيه كيفية تثبيت البرامج الثابتة rom التحميل ...\n",
              "6                            وأنه هو أغنى وأقنى النجم  \n",
              "7     اللهم قدر لنا الفرح بكل اشكاله انت الكريم الذي...\n",
              "8      غزه_تحت_القصف داعش أخواني حيل عندكم بالمدنيين...\n",
              "9      يعلمون ظاهرا من الحياة الدنيا وهم عن الآخرة ه...\n",
              "10               افضل كتاب قرأته هو أمي ابراهام لنكولن \n",
              "11    ولأن ه م م لائ ك ة ص غار ن ع ش ق ات كاءة رؤوس ...\n",
              "12        خ لاصة الح ب هي ت فكر بقلبهآ وهو ي فكر بعقله \n",
              "13    جميل آن يفهمك من تحبب ويخآف عليك و يغآر عليكك ...\n",
              "14    حتى الندم على المعصيه تؤجر عليه سبحانك يالله م...\n",
              "15     اها يا بيبي والله اتهرست علي تويتر و ع الفيس ...\n",
              "16     لا يقاتلونكم جميعا إلا في قرى محصنة أو من ورا...\n",
              "17          طبت منك نهائيا بس كلي رجاء ما ترجع من جديد \n",
              "18     وإن تجهر بالقول فإنه يعلم السر وأخفى طه  تطبي...\n",
              "19                                                  الو\n",
              "20             داعش يصلب سوريا  ساعات وهو حي داعش syria\n",
              "21    ﺧﻟك ﻋزﯾز آﻟﻧﻓس ﻟۈ ﮪﻣۈﻣك ﺟﺑآللاﭠﺷﺷﮐي ﻟﻟﻧآﺳس ﻣن ...\n",
              "22    اللهم أفرح قلبي وقلب من أحب وأغسل أحزاننا وهمو...\n",
              "23    اللهم أحسن عاقبتنا في الأمور كلها وأجرنا من خز...\n",
              "24     سيدي إيفني شكاية أجنبي بدرك ميرلفت يحرك فرقة ...\n",
              "25     الصوره هاذي الله يسلمك في اسبانبا غير صحيح ان...\n",
              "26                             انتظري اجل خيره لك يارب \n",
              "27               اللهم اسعد امي كما تسعدني و احفظها لي \n",
              "28     إن الذين كفروا من أهل الكتاب والمشركين في نار...\n",
              "29    اللهم أعز الإسلآم والمسلمين اللهم زلزل الأرض م...\n",
              "30            الوفاء أن تدعو ل من ت حب حتى وإن رحل عنك \n",
              "31               اللهم اعني على ذكرك وشكرك وحسن عبادتك \n",
              "32    ل ي ك ف ر وا ب م ا آت ي ن اه م ف ت م ت ع وا ف ...\n",
              "33                             وش حطيتي انا اقول تيفاني\n",
              "34        اللهم حقق لي أمنيه همست بها الى السماء كثيرا \n",
              "35    veto فيتو شـــــاهد الغارات الاسرائيلية تضرب غ...\n",
              "36     هيياااا عبدالسلاااام يااا عقاااااليييي للحب_كلمة\n",
              "37                               هههههه ماشاء الله حكيك\n",
              "38     الله لا إله إلا هو وعلى الله فليتوكل المؤمنون...\n",
              "39                              من زمان نبي المقطع هذا \n",
              "40    يا معشر الشباب من استطاع منكم الباءة فليتزوج ف...\n",
              "41     اتوقع انكم ماحولكم لا موهبه ولا حولكم فن اللي...\n",
              "42    إضغط على منطقتك يتبين لك كم يتبقى من الوقت عن ...\n",
              "43    انقلاب حافلة في قلي ب خضر بالجوف الى إصابة  أش...\n",
              "44    ان كان غيري عن هوى قلبي اغناك يغنيني الله عن ه...\n",
              "45    الل ه م ص ل و س ل م ع لى ن ب ي ن آ م ح م د و ع...\n",
              "46     الغاليه والقديرة الشاعرة اسرار اختي اللتي لم ...\n",
              "47    اقل مافيها الشيخ فيصل المالك ادي ٥ بالميه من و...\n",
              "48                                             دي ماريا\n",
              "49    ح س ب ـي الل ه لا إله إلا ه و ع ل ـيه ت و ك ـل...\n",
              "Name: tweet, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3K1zxZpT1r_y",
        "outputId": "b8a3f839-9e3e-4506-bc21-caac9b5730be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "\n",
        "X_train = df_train.tweet\n",
        "y_train = df_train.label\n",
        "X_test = df_test.tweet\n",
        "y_test = df_test.label\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(52325,)\n",
            "(52325,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxIOQJawTg10"
      },
      "source": [
        "# from sklearn.model_selection import train_test_split             \n",
        "# from keras.preprocessing.text import Tokenizer                    \n",
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# tokenizer = Tokenizer(num_words=5000)\n",
        "# tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# X_X_train = tokenizer.texts_to_sequences(X_train)\n",
        "# X_X_test = tokenizer.texts_to_sequences(X_test)\n",
        "# # Adding 1 because of  reserved 0 index\n",
        "# vocab_size = len(tokenizer.word_index) + 1                          \n",
        "\n",
        "# maxlen = 100\n",
        "\n",
        "# X_training = pad_sequences(X_X_train, padding='post', maxlen=maxlen)\n",
        "# X_testing = pad_sequences(X_X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "# X_training.shape\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5pWJlm1Wx4p"
      },
      "source": [
        "# from keras.models import Sequential\n",
        "# from keras import layers\n",
        "# embedding_dim = 100\n",
        "\n",
        "# model = Sequential()\n",
        "# model.add(layers.Embedding(vocab_size, 100, input_length=maxlen))\n",
        "# model.add(layers.Conv1D(128, 5, activation='relu'))\n",
        "# model.add(layers.GlobalMaxPooling1D())\n",
        "# model.add(layers.Dense(10, activation='relu'))\n",
        "# model.add(layers.Dense(1, activation='softmax'))\n",
        "# model.compile(optimizer='adam',\n",
        "#               loss='categorical_crossentropy',\n",
        "#               metrics=['accuracy'])\n",
        "# history = model.fit(X_training, y_train,\n",
        "#                     epochs=10,\n",
        "#                     validation_data=(X_testing, y_test),\n",
        "#                     batch_size=10)  \n",
        "\n",
        "# history"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1Ry7tfIeHLt",
        "outputId": "717f9433-0fe4-4a47-f9ce-4fe63377e35e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import itertools\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import utils\n",
        "\n",
        "train_posts = df_train.tweet\n",
        "train_tags = df_train.label\n",
        "test_posts = df_test.tweet\n",
        "test_tags = df_test.label\n",
        "\n",
        "max_words = 1000\n",
        "tokenize = text.Tokenizer(num_words=max_words, char_level=False)\n",
        "tokenize.fit_on_texts(train_posts) # only fit on train\n",
        "\n",
        "x_train = tokenize.texts_to_matrix(train_posts)\n",
        "x_test = tokenize.texts_to_matrix(test_posts)\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(train_tags)\n",
        "y_train = encoder.transform(train_tags)\n",
        "y_test = encoder.transform(test_tags)\n",
        "\n",
        "num_classes = np.max(y_train) + 1\n",
        "y_train = utils.to_categorical(y_train, num_classes)\n",
        "y_test = utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 2\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(max_words,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "              \n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_split=0.1)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "1472/1472 [==============================] - 4s 3ms/step - loss: 0.7296 - accuracy: 0.8203 - val_loss: 9.5068 - val_accuracy: 0.0023\n",
            "Epoch 2/2\n",
            "1472/1472 [==============================] - 4s 3ms/step - loss: 0.5191 - accuracy: 0.8736 - val_loss: 11.2793 - val_accuracy: 0.0023\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_o2diDwfnFD",
        "outputId": "0a3245e6-e5fd-4750-8b31-76d9f2a27a5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "score = model.evaluate(x_test, y_test,\n",
        "                       batch_size=batch_size, verbose=1)\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "413/413 [==============================] - 1s 2ms/step - loss: 1.6949 - accuracy: 0.7786\n",
            "Test accuracy: 0.778620183467865\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}